{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to do:\n",
    "# Glean a small portion of the data (10k?), each one arbitrarily treating one song as the target and removing that from the playlist, applying a scaler first.\n",
    "# Design RNN (first do a basic NN off of summary) to accept variable number of inputs and get one output of a number of variables\n",
    "# train RNN. RNN does not get used to pick the songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "with open(\"../data/playlist_data.json\") as jsonfile:\n",
    "    playlist_data = json.load(jsonfile)\n",
    "with open(\"../data/track_audio_data.json\") as jsonfile:\n",
    "    track_data = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Am going to need every track, only 10k playlists.\n",
    "# Still should make a dict of tracks in playlists to see how many appear.\n",
    "import random\n",
    "sample_size = 10000\n",
    "playlist_samples = random.sample(playlist_data, sample_size)\n",
    "# Use pid to get summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORD_FEATURES = [\"danceability\", \"energy\", \"loudness\", \"mode\", \"speechiness\",\n",
    "            \"acousticness\", \"liveness\", \"valence\", \"tempo\", \"duration_ms\"]\n",
    "\n",
    "CAT_FEATURES = {\n",
    "    \"time_signature\": [\"0\", \"1\", \"3\", \"4\", \"5\"],\n",
    "    \"key\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\"]\n",
    "}\n",
    "\n",
    "def playlist_summary(playlist):\n",
    "    total_tracks = playlist[\"num_tracks\"]\n",
    "    audio_summary = dict.fromkeys(ORD_FEATURES, 0)\n",
    "    for feat, values in CAT_FEATURES.items():\n",
    "        for value in values:\n",
    "            audio_summary[f\"{feat}_{value}\"] = 0\n",
    "    for track_num in playlist[\"tracks\"]:\n",
    "        try:\n",
    "            track_audio_feats = track_data[str(track_num)]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        if track_audio_feats is None:\n",
    "            total_tracks -= 1\n",
    "            continue\n",
    "        for feat, val in track_audio_feats.items():\n",
    "            if feat in CAT_FEATURES:\n",
    "                audio_summary[f\"{feat}_{val}\"] += 1\n",
    "            elif feat in ORD_FEATURES:\n",
    "                audio_summary[feat] += val\n",
    "            \n",
    "    for feat in audio_summary.keys():\n",
    "        audio_summary[feat] = round(audio_summary[feat] / total_tracks, 5)\n",
    "    return audio_summary\n",
    "\n",
    "def single_track_summary(track):\n",
    "    audio_summary = dict.fromkeys(ORD_FEATURES, 0)\n",
    "    for feat, values in CAT_FEATURES.items():\n",
    "        for value in values:\n",
    "            audio_summary[f\"{feat}_{value}\"] = 0\n",
    "    track_audio_feats = track_data[str(track)]\n",
    "    for feat, val in track_audio_feats.items():\n",
    "        if feat in CAT_FEATURES:\n",
    "            audio_summary[f\"{feat}_{val}\"] += 1\n",
    "        elif feat in ORD_FEATURES:\n",
    "            audio_summary[feat] += val\n",
    "    return audio_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playlist_x_y(playlist):\n",
    "    masked_playlist = dict(playlist)\n",
    "    masked_playlist['tracks'] = playlist['tracks'].copy()\n",
    "    y_track = masked_playlist['tracks'].pop(random.randrange(playlist['num_tracks']))\n",
    "    masked_playlist['num_tracks'] -= 1\n",
    "    return playlist_summary(masked_playlist), single_track_summary(y_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = [p['pid'] for p in playlist_samples]\n",
    "sample_tracks = [p['tracks'] for p in playlist_samples]\n",
    "sample_summaries = [playlist_summary(p) for p in playlist_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_list, y_list = [], []\n",
    "for sample in playlist_samples:\n",
    "    x, y = playlist_x_y(sample)\n",
    "    x_list.append(list(x.values()))\n",
    "    y_list.append(list(y.values()))\n",
    "\n",
    "sample_x = np.array(x_list, dtype=np.float64)\n",
    "sample_y = np.array(y_list, dtype=np.float64)\n",
    "num_feats = sample_y.shape[1]\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaled_x = scaler.fit_transform(sample_x)\n",
    "# scaled_y = scaler.transform(sample_y)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(sample_x, sample_y)\n",
    "\n",
    "# Use inverse_transform when applying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class PlaylistNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PlaylistNet, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(num_feats, 50, dtype=torch.float64)\n",
    "        self.output_layer = nn.Linear(50, num_feats, dtype=torch.float64)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.input_layer(x))\n",
    "        # x = torch.relu(self.hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class PlaylistDataset(Dataset):\n",
    "    def __init__(self, playlists, targets):\n",
    "        self.playlists = playlists\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.playlists)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.playlists[idx], self.targets[idx]\n",
    "\n",
    "model = PlaylistNet()\n",
    "\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "train_data = PlaylistDataset(train_x, train_y)\n",
    "valid_data = PlaylistDataset(valid_x, valid_y)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 1: 657111.1436185674\n",
      "Validation loss at epoch 1: 218775.5338209781\n",
      "Training loss at epoch 2: 657058.910320662\n",
      "Validation loss at epoch 2: 218760.53778420188\n",
      "Training loss at epoch 3: 657014.0586577792\n",
      "Validation loss at epoch 3: 218745.63040480617\n",
      "Training loss at epoch 4: 656969.4306808783\n",
      "Validation loss at epoch 4: 218730.74463707945\n",
      "Training loss at epoch 5: 656924.6452942094\n",
      "Validation loss at epoch 5: 218715.81163485823\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Training loss at epoch {epoch + 1}: {running_loss}\")\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    for i, data in enumerate(valid_loader):\n",
    "        inputs, targets = data\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        running_vloss += loss.item()\n",
    "    print(f\"Validation loss at epoch {epoch + 1}: {running_vloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59042, 0.72268, -6.26432, 0.60526, 0.05313, 0.13094, 0.18722, 0.46711, 113.19379, 239733.26316, 0.0, 0.0, 0.07895, 0.92105, 0.0, 0.07895, 0.23684, 0.05263, 0.0, 0.10526, 0.07895, 0.05263, 0.10526, 0.05263, 0.15789, 0.02632, 0.05263]\n",
      "[0.6293630483626644, 0.6752987499242384, -6.506638996377977, 0.9971395727775517, 0.0590685016063939, 0.10381959210899479, 0.1287376637102075, 0.47131849530970976, 39.60992694265453, 39.27887272922309, -0.030177136103844773, -0.003578005534715392, 0.0037794819342839917, 0.9876365290202315, 0.011548265363761315, -0.015178420940060972, -0.026062852202577383, 0.017181676312142263, 0.007820147387078022, 0.00028877383282177593, 0.015417293462005766, -0.011544718988168463, 0.008132012049267438, -0.030710337618963898, -0.003371229428191125, 0.00582371096691925, 0.030470169090965574]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "values = list(playlist_summary(playlist_samples[0]).values())\n",
    "print(values)\n",
    "x = torch.tensor(values, dtype=torch.float64)\n",
    "print(model(x).tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
